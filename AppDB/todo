use metadata key to invalidate project_id and composite index defs
add project_id validation to protobuf (and future) interfaces
define indexes via API rather than shared zk state
retry operations when they fail

----
add garbage collection:

      if old_version != self._ABSENT_VERSION:
        project_id = key.app()
        namespace = key.name_space()
        self._gc.index_deleted_version(tr, project_id, namespace, path,
                                       old_version, op_id=index)

versionstamp_future = tr.get_versionstamp()
# yield self._tornado_fdb.commit(tr)
self._gc.clear_later(project_id, namespace, path, old_version,
                           versionstamp_future)
----

get rid of txid int handles (standardize on bytestring handles from public api)
enforce 60-sec tx time
split read keys in tx metadata if it exceeds 10k
check for limits (max keys for get request, max entities in put, etc)
check what the correct v3 response is when putting entities in a tx (include versions?)

---

generate stats (atomic operations?)

garbage: used by the garbage collector to manage deleted data
garbage: See the GarbageCollector class for more details.

---
    self._gc = None
---
    gc_lock = PollingLock(
      self._db, self._tornado_fdb, ds_dir.pack((GarbageCollector.LOCK_KEY,)))
    gc_lock.start()

    self._gc = GarbageCollector(
      self._db, self._tornado_fdb, gc_lock, self._directory_cache)
    self._gc.start()
---

    # Map commit versionstamp to entity version.
    versions_dir = self._directory_cache.get((project_id, self._VERSIONS_DIR,
                                              namespace))
    key = versions_dir.pack_with_versionstamp(
      path + (fdb.tuple.Versionstamp(),))
    tr.set_versionstamped_key(key, fdb.tuple.pack((new_version,)))

---

      versions_dir = self._directory_cache.get((project_id, self._VERSIONS_DIR,
                                                namespace))
      versions_range = versions_dir.range(path)
      versions_range = slice(versions_range.start,
                             versions_dir.pack(path + (read_vs,)))
      kvs, count, more_results = yield self._tornado_fdb.get_range(
        tr, versions_range, limit=1, reverse=True)
      if not count:
        raise gen.Return((None, self._ABSENT_VERSION))

---

rework directory cache. per-project?

---

handle pointvalue, uservalue, referencevalue
test decoding/encoding double (float)
scattered property
test delete response (pretty sure popluated version field is the new empty version?)

---

remove txid handle mapping (do it in the api server)
support IN and EXISTS filters?
test invalidation for ancestor queries (does GAE invalidate for queries in the
whole group, or can it deal with ranges?)
add tests for functionality that doesn't work in current datastore
 - snapshot isolation
 - queries with different levels of ancestors
 - prop order (empty values)
 - entity versions
 - projection (performance?)
support advanced queries: https://cloud.google.com/appengine/articles/indexselection
test if prop result (projection) specifies group
extend fdb.tuple for custom data types and reversable data types
fix issue where query cursor can point to the next result, which may be an
updated index value (need to exclude whole value instead of just key)
clean up merge join iterator. handle ancestors and cursors there.
what's up with the cursor for every result? (pb response from sdk and gae)
TestCursorWithZigZagMergeHandler doesn't seem to actually use a cursor
entity groups not in all results
wtf is up with defining an offset + a cursor?
refine data layer: commit_vs shouldn't equal 0 when entity doesn't exist
fix fdb python bug with encoding versionstamp for fdb.impl.Value
transactional tasks

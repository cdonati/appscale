use metadata key to invalidate project_id and composite index defs
add project_id validation to protobuf (and future) interfaces
define indexes via API rather than shared zk state
retry operations when they fail

----
add garbage collection:

      if old_version != self._ABSENT_VERSION:
        project_id = key.app()
        namespace = key.name_space()
        self._gc.index_deleted_version(tr, project_id, namespace, path,
                                       old_version, op_id=index)

versionstamp_future = tr.get_versionstamp()
# yield self._tornado_fdb.commit(tr)
self._gc.clear_later(project_id, namespace, path, old_version,
                           versionstamp_future)
----

get rid of txid int handles (standardize on bytestring handles from public api)
enforce 60-sec tx time
split read keys in tx metadata if it exceeds 10k
check for limits (max keys for get request, max entities in put, etc)
check what the correct v3 response is when putting entities in a tx (include versions?)

---

generate stats (atomic operations?)

garbage: used by the garbage collector to manage deleted data
garbage: See the GarbageCollector class for more details.

---
    self._gc = None
---
    gc_lock = PollingLock(
      self._db, self._tornado_fdb, ds_dir.pack((GarbageCollector.LOCK_KEY,)))
    gc_lock.start()

    self._gc = GarbageCollector(
      self._db, self._tornado_fdb, gc_lock, self._directory_cache)
    self._gc.start()
---

    # Map commit versionstamp to entity version.
    versions_dir = self._directory_cache.get((project_id, self._VERSIONS_DIR,
                                              namespace))
    key = versions_dir.pack_with_versionstamp(
      path + (fdb.tuple.Versionstamp(),))
    tr.set_versionstamped_key(key, fdb.tuple.pack((new_version,)))

---

      versions_dir = self._directory_cache.get((project_id, self._VERSIONS_DIR,
                                                namespace))
      versions_range = versions_dir.range(path)
      versions_range = slice(versions_range.start,
                             versions_dir.pack(path + (read_vs,)))
      kvs, count, more_results = yield self._tornado_fdb.get_range(
        tr, versions_range, limit=1, reverse=True)
      if not count:
        raise gen.Return((None, self._ABSENT_VERSION))

---

rework directory cache. per-project?

---

enforce xg constraint

    # Fail if there are too many groups involved in the transaction.
    groups_mutated = set()
    for mutation in mutations:
      
    groups_put = {group_for_key(key).Encode() for key in metadata['puts']}
    groups_deleted = {group_for_key(key).Encode()
                      for key in metadata['deletes']}
    groups_mutated = groups_put | groups_deleted
    tx_groups = groups_mutated | metadata['reads']
    if len(tx_groups) > dbconstants.MAX_GROUPS_FOR_XG:
      raise dbconstants.TooManyGroupsException(
        'Too many groups in transaction')
---

test behavior of queries including meaning field

---

handle pointvalue, uservalue, referencevalue
test decoding/encoding double (float)
scattered property
test delete response (pretty sure popluated version field is the new empty version?)
improve merge join query performance
make sure tmp_chunks works correctly

---

remove txid handle mapping (do it in the api server)
support IN and EXISTS filters
test invalidation for ancestor queries (does GAE invalidate for queries in the
whole group, or can it deal with ranges?)
add tests for functionality that doesn't work in current datastore
 - snapshot isolation
 - queries with different levels of ancestors
 - prop order (empty values)
support advanced queries: https://cloud.google.com/appengine/articles/indexselection
test if prop result (projection) specifies group
